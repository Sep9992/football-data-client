# ml/step3_2_train_xgboost.py
# TRÃ‰NINK MODELU: XGBoost Classifier (The Ferrari) ğŸï¸
# CÃ­l: Porazit Random Forest v pÅ™esnosti a Log Loss.

import os
import pandas as pd
import joblib
from sqlalchemy import create_engine
from dotenv import load_dotenv
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score, classification_report, log_loss
from sklearn.calibration import CalibratedClassifierCV
from xgboost import XGBClassifier

# NastavenÃ­
pd.set_option('display.max_columns', None)
pd.set_option('display.width', 1000)

load_dotenv()
DATABASE_URL = os.getenv("DATABASE_URL")
engine = create_engine(DATABASE_URL)
DATA_DIR = os.path.join(os.path.dirname(os.path.dirname(__file__)), "data")
MODEL_PATH = os.path.join(DATA_DIR, "model_xgboost_ferrari.pkl")

# Importujeme featury ze sdÃ­lenÃ©ho souboru
from ml.shared_features import performance_features


def train_xgboost():
    print("ğŸš€ Startuji trÃ©nink XGBoost (The Ferrari)...")

    # 1. NaÄtenÃ­ dat
    print("â³ NaÄÃ­tÃ¡m data z DB...")
    df = pd.read_sql("SELECT * FROM prepared_datasets", engine)
    df = df.dropna(subset=["target"]).reset_index(drop=True)

    # PouÅ¾ijeme stejnÃ© featury jako u RF
    X = df[performance_features]
    y = df["target"].astype(int)

    print(f"ğŸ“Š Dataset: {len(df)} Å™Ã¡dkÅ¯, {len(performance_features)} features")

    # 2. RozdÄ›lenÃ­ (ÄŒasovÃ©, ne nÃ¡hodnÃ© - aby se neuÄil z budoucnosti)
    # PrvnÃ­ch 80% zÃ¡pasÅ¯ na trÃ©nink, poslednÃ­ch 20% na test
    split_idx = int(len(df) * 0.80)
    X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]
    y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]

    print(f"âœ‚ï¸ Train: {len(X_train)} | Test: {len(X_test)}")

    # 3. Definice Modelu (XGBoost)
    # Parametry nastaveny konzervativnÄ› pro fotbal (prevence overfittingu)
    xgb = XGBClassifier(
        n_estimators=150,  # PoÄet stromÅ¯
        learning_rate=0.05,  # PomalejÅ¡Ã­ uÄenÃ­ = lepÅ¡Ã­ stabilita
        max_depth=4,  # MenÅ¡Ã­ hloubka = mÃ©nÄ› overfittingu
        subsample=0.8,  # Bere jen 80% dat pro kaÅ¾dÃ½ strom (Å¡um)
        colsample_bytree=0.8,  # Bere jen 80% featur pro kaÅ¾dÃ½ strom
        objective='multi:softprob',
        random_state=42,
        eval_metric='mlogloss',
        n_jobs=-1
    )

    # DÅ®LEÅ½ITÃ‰: Kalibrace pravdÄ›podobnostÃ­
    # XGBoost si Äasto vÄ›Å™Ã­ moc (napÅ™. 0.99). Kalibrace ho vrÃ¡tÃ­ na zem (napÅ™. 0.85).
    calibrated_xgb = CalibratedClassifierCV(xgb, method='isotonic', cv=3)

    # 4. Pipeline
    model_pipeline = Pipeline([
        ('imputer', SimpleImputer(strategy='mean')),  # XGB umÃ­ NaN, ale imputer je jistota
        ('scaler', StandardScaler()),  # XGB nepotÅ™ebuje Å¡kÃ¡lovÃ¡nÃ­, ale pomÃ¡hÃ¡ konvergenci
        ('clf', calibrated_xgb)
    ])

    # 5. TrÃ©nink
    print("ğŸ§  TrÃ©nuji model...")
    model_pipeline.fit(X_train, y_train)

    # 6. Evaluace
    print("ğŸ“ˆ Vyhodnocuji na testovacÃ­ch datech...")
    y_pred = model_pipeline.predict(X_test)
    y_proba = model_pipeline.predict_proba(X_test)

    acc = accuracy_score(y_test, y_pred)
    loss = log_loss(y_test, y_proba)

    print("\n" + "=" * 40)
    print(f"ğŸ† VÃSLEDKY XGBoost")
    print(f"âœ… Accuracy: {acc:.4f} (NÃ¡hodnÃ½ tip = 0.33)")
    print(f"ğŸ“‰ Log Loss: {loss:.4f} (ÄŒÃ­m mÃ©nÄ›, tÃ­m lÃ©pe)")
    print("=" * 40)
    print("\nReport klasifikace:")
    print(classification_report(y_test, y_pred, target_names=["Home", "Draw", "Away"]))

    # 7. Feature Importance (Trochu sloÅ¾itÄ›jÅ¡Ã­ u Pipeline + Calibrated)
    # VytÃ¡hneme vnitÅ™nÃ­ model
    try:
        base_xgb = model_pipeline.named_steps['clf'].calibrated_classifiers_[0].estimator
        importances = base_xgb.feature_importances_
        feature_imp = pd.DataFrame({'Feature': performance_features, 'Importance': importances})
        feature_imp = feature_imp.sort_values(by='Importance', ascending=False).head(10)
        print("\nğŸ” TOP 10 NejdÅ¯leÅ¾itÄ›jÅ¡Ã­ch faktorÅ¯:")
        print(feature_imp.to_string(index=False))
    except:
        print("\nâš ï¸ Feature importance nelze zobrazit u kalibrovanÃ©ho modelu jednoduÅ¡e.")

    # 8. UloÅ¾enÃ­
    # UklÃ¡dÃ¡me to jako NOVÃ soubor, nepÅ™episujeme ten starÃ½!
    joblib.dump(model_pipeline, MODEL_PATH)
    print(f"\nğŸ’¾ Model uloÅ¾en do: {MODEL_PATH}")
    print("âœ… Hotovo. NynÃ­ mÅ¯Å¾ete porovnat s Random Forest.")


if __name__ == "__main__":
    train_xgboost()